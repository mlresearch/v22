<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

<link rel="alternate" type="application/rss+xml" href="http://www.jmlr.org/jmlr.xml" title="JMLR RSS">
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
<style type="text/css">
<!-- 
#fixed {
    position: absolute;
    top: 0;
    left: 0;
    width: 8em;
    height: 100%;
}
body > #fixed {
    position: fixed;
}
#content {
    margin-top: 1em;
    margin-left: 10em;
    margin-right: 0.5em;
}
img.jmlr {
    width: 7em;
}
img.rss {
    width: 2em;
}
-->
</style>
<script language="JavaScript"> 
<!-- function GoAddress(user,machine) {
document.location = 'mailto:' + user + '@' + machine; } 
// -->
</script>


<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>

<div id="content">
<h2>The adversarial stochastic shortest path problem with unknown transition probabilities</h2><p><i><b>Gergely Neu, Andras Gyorgy, Csaba Szepesvari </b></i>; JMLR W&ampCP 22: 805-813, 2012.</p><h3>Abstract</h3><p>We consider online learning in a special class of episodic Markovian decision processes, namely, loop-free stochastic shortest path problems. In this problem, an agent has to traverse through a finite directed acyclic graph with random transitions while maximizing the obtained rewards along the way. We assume that the reward function can change arbitrarily between consecutive episodes, and is entirely revealed to the agent at the end of each episode.
Previous work was concerned with the case when the stochastic dynamics is known ahead of time, 
whereas the main novelty of this paper is that this assumption is lifted.
We propose an algorithm called ``follow the perturbed optimistic policy'' that combines ideas from the ``follow the perturbed leader''  method for online learning of arbitrary sequences and ``upper confidence reinforcement learning'', an algorithm for regret minimization in Markovian decision processes (with a fixed reward function). We prove that the expected cumulative regret of our algorithm is of order $L
X
A\sqrt{T}$ up to logarithmic factors, where $L$ is the length of the longest path in the graph, $\X$ is the state space, $\A$ is the action space and $T$ is the number of episodes.
To our knowledge this is the first algorithm that learns and controls stochastic and adversarial components in an online fashion at the same time.
</p>
</div>
 <div id="fixed">
<br>
<a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.csail.mit.edu/jmlr.jpg" align="right" border="0"></a>
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.csail.mit.edu/RSS.gif" class="rss" alt="RSS Feed">
</a>



</p></div>
 
<p></p><center>Page last modified on Thu April 26 2012 13:56 2012.</center>

<p> 

<table width="100%"> <tbody><tr>
<td align="right"><font size="-1">Copyright 
@ <a target="_top" href="http://www.jmlr.org/">JMLR</a> 2012.  All rights
reserved.</font></td> </tr> </tbody></table>
</p></body></html>
