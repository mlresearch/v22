---
title: Closed-Form Entropy Limits -  A Tool to Monitor Likelihood Optimization of
  Probabilistic Generative Models
abstract: The maximization of the data likelihood under a given probabilistic generative
  model is the essential goal of many algorithms for unsupervised learning. If expectation
  maximization is used for optimization, a lower bound on the data likelihood, the
  free-energy, is optimized. The parameter-dependent part of the free-energy (the
  difference between free-energy and posterior entropy) is the essential entity in
  the derivation of learning algorithms. Here we show that for many common generative
  models the optimal values of the parameter-dependent part can be derived in closed-form.
  These closed-form expressions are hereby given as sums of the negative (differential)
  entropies of the individual model distributions. We apply our theoretical results
  to derive such closed-form expressions for a number of common and recent models,
  including probabilistic PCA, factor analysis, different versions of sparse coding,
  and Linear Dynamical Systems. The main contribution of this work is theoretical
  but we show how the derived results can be used to efficiently compute free-energies,
  and how they can be used for consistency checks of learning algorithms.
pdf: http://proceedings.mlr.press/v22/lucke12/lucke12.pdf
layout: inproceedings
id: lucke12
month: 0
firstpage: 731
lastpage: 740
page: 731-740
sections: 
author:
- given: Jorg
  family: Lucke
- given: Marc
  family: Henniges
date: 2012-03-21
address: La Palma, Canary Islands
publisher: PMLR
container-title: Proceedings of the Fifteenth International Conference on Artificial
  Intelligence and Statistics
volume: '22'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 3
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
