---
title: Using More Data to Speed-up Training Time
abstract: In many recent applications, data is plentiful. By now, we have a rather
  clear understanding of how more data can be used to improve the accuracy of learning
  algorithms. Recently, there has been a growing interest in understanding how more
  data can be leveraged to reduce the required training runtime. In this paper, we
  study the runtime of learning as a function of the number of available training
  examples, and underscore the main high-level techniques. We provide the first formal
  positive result showing that even in the unrealizable case, the runtime can decrease
  exponentially while only  requiring a polynomial growth of the number of examples.
  Our construction corresponds to a synthetic learning problem and an interesting
  open question is whether the tradeoff can be shown for more natural learning problems.
  We spell out several interesting candidates of natural learning problems for which
  we conjecture that there is a tradeoff between computational and sample complexity.
pdf: http://proceedings.pmlr.press/shalev-shwartz12/shalev-shwartz12.pdf
supplementary: Supplementary:http://jmlr.org/proceedings/papers/v22/shalev-shwartz12/shalev-shwartz12Supple.pdf
layout: inproceedings
id: shalev-shwartz12
month: 0
firstpage: 1019
lastpage: 1027
page: 1019-1027
origpdf: http://jmlr.org/proceedings/papers/v22/shalev-shwartz12/shalev-shwartz12.pdf
sections: 
author:
- given: Shai
  family: Shalev-Shwartz
- given: Ohad
  family: Shamir
- given: Eran
  family: Tromer
date: 2012-03-21
address: La Palma, Canary Islands
publisher: PMLR
container-title: Proceedings of the Fifteenth International Conference on Artificial
  Intelligence and Statistics
volume: '22'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 3
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
