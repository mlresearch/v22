---
title: Stochastic Bandit Based on Empirical Moments
abstract: In the multiarmed bandit problem a gambler chooses an arm of a slot machine
  to pull considering a tradeoff between exploration and exploitation. We study the
  stochastic bandit problem where each arm has a reward distribution supported in
  [0,1]. For this model, there exists a policy which achieves the theoretical bound
  asymptotically. However the optimal policy requires a computation of a convex optimization
  which involves the empirical distribution of each arm. In this paper, we propose
  a policy which exploits the first d empirical moments for arbitrary d fixed in advance.
  We show that the performance of the policy approaches the theoretical bound as d
  increases. This policy can be implemented by solving polynomial equations, which
  we derive the explicit solution for d smaller than 5. By choosing appropriate d,
  the proposed policy realizes a tradeoff between the computational complexity and
  the expected regret.
pdf: "./honda12/honda12.pdf"
supplementary: Supplementary:http://jmlr.org/proceedings/papers/v22/honda12/honda12Supple.pdf
layout: inproceedings
id: honda12
month: 0
firstpage: 529
lastpage: 537
page: 529-537
origpdf: http://jmlr.org/proceedings/papers/v22/honda12/honda12.pdf
sections: 
author:
- given: Junya
  family: Honda
- given: Akimichi
  family: Takemura
date: '2012-03-21 00:08:49'
publisher: PMLR
---
