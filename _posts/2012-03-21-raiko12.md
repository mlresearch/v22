---
title: Deep Learning Made Easier by Linear Transformations in Perceptrons
abstract: We transform the outputs of each hidden neuron in a multi-layer perceptron
  network to have zero activation and zero slope on average, and use separate shortcut
  connections to model the linear dependencies instead. This transformation aims at
  separating the problems of learning the linear and nonlinear parts of the whole
  input-output mapping, which has many benefits. We study the theoretical properties
  of the transformation by noting that they make the Fisher information matrix closer
  to a diagonal matrix, and thus standard gradient closer to the natural gradient.
  We experimentally confirm the usefulness of the transformations by noting that they
  make basic stochastic gradient learning competitive with state-of-the-art learning
  algorithms in speed, and that they seem also to help find solutions that generalize
  better. The experiments include both classification of small images and learning
  a low-dimensional representation for images by using a deep unsupervised auto-encoder
  network. The transformations were beneficial in all cases, with and without regularization
  and with networks from two to five hidden layers.
pdf: "./raiko12/raiko12.pdf"
layout: inproceedings
id: raiko12
month: 0
firstpage: 924
lastpage: 932
page: 924-932
origpdf: http://jmlr.org/proceedings/papers/v22/raiko12/raiko12.pdf
sections: 
author:
- given: Tapani
  family: Raiko
- given: Harri
  family: Valpola
- given: Yann
  family: Lecun
date: '2012-03-21 00:15:24'
publisher: PMLR
---
