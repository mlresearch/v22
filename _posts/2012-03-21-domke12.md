---
title: Generic Methods for Optimization-Based Modeling
abstract: "\"Energy” models for continuous domains can be applied to many problems,
  but often suffer from high computational expense in training, due to the need to
  repeatedly minimize the energy function to high accuracy. This paper considers a
  modified setting, where the model is trained in terms of results after optimization
  is truncated to a fixed number of iterations. We derive “backpropagating” versions
  of gradient descent, heavy-ball and LBFGS. These are simple to use, as they require
  as input only routines to compute the gradient of the energy with respect to the
  domain and parameters. Experimental results on denoising and image labeling problems
  show that learning with truncated optimization greatly reduces computational expense
  compared to “full” fitting."
pdf: http://proceedings.mlr.press/v22/domke12/domke12.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: domke12
month: 0
tex_title: Generic Methods for Optimization-Based Modeling
firstpage: 318
lastpage: 326
page: 318-326
order: 318
cycles: false
author:
- given: Justin
  family: Domke
date: 2012-03-21
address: La Palma, Canary Islands
publisher: PMLR
container-title: Proceedings of the Fifteenth International Conference on Artificial
  Intelligence and Statistics
volume: '22'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 3
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
