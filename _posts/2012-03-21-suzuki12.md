---
title: 'Fast Learning Rate of Multiple Kernel Learning: Trade-Off between Sparsity
  and Smoothness'
abstract: We investigate the learning rate of multiple kernel leaning (MKL) with L1
  and elastic-net regularizations. The elastic-net regularization is a composition
  of an L1-regularizer for inducing the sparsity and an L2-regularizer for controlling
  the smoothness. We focus on a sparse setting where the total number of kernels is
  large but the number of non-zero components of the ground truth is relatively small,
  and show sharper convergence rates than the learning rates ever shown for both L1
  and elastic-net regularizations. Our analysis shows there appears a trade-off between
  the sparsity and the smoothness when it comes to selecting which of L1 and elastic-net
  regularizations to use; if the ground truth is smooth, the elastic-net regularization
  is preferred, otherwise the L1 regularization is preferred.
pdf: http://proceedings.mlr.press/v22/suzuki12/suzuki12.pdf
layout: inproceedings
id: suzuki12
month: 0
firstpage: 1152
lastpage: 1183
page: 1152-1183
sections: 
author:
- given: Taiji
  family: Suzuki
- given: Masashi
  family: Sugiyama
date: 2012-03-21
address: La Palma, Canary Islands
publisher: PMLR
container-title: Proceedings of the Fifteenth International Conference on Artificial
  Intelligence and Statistics
volume: '22'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 3
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
