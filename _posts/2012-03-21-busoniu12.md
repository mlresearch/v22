---
title: Optimistic planning for Markov decision processes
abstract: The reinforcement learning community has recently intensified its interest
  in online planning methods, due to their relative independence on the state space
  size. However, tight near-optimality guarantees are not yet available for the general
  case of stochastic Markov decision processes and closed-loop, state-dependent planning
  policies. We therefore consider an algorithm related to AO* that optimistically
  explores a tree representation of the space of closed-loop policies, and we analyze
  the near-optimality of the action it returns after n tree node expansions. While
  this optimistic planning requires a finite number of actions and possible next states
  for each transition, its asymptotic performance does not depend directly on these
  numbers, but only on the subset of nodes that significantly impact near-optimal
  policies. We characterize this set by introducing a novel measure of problem complexity,
  called the near-optimality exponent. Specializing the exponent and performance bound
  for some interesting classes of MDPs illustrates the algorithm works better when
  there are fewer near-optimal policies and less uniform transition probabilities.
pdf: "./busoniu12/busoniu12.pdf"
supplementary: Supplementary:http://jmlr.org/proceedings/papers/v22/busoniu12/busoniu12Supple.pdf
layout: inproceedings
id: busoniu12
month: 0
firstpage: 182
lastpage: 189
page: 182-189
origpdf: http://jmlr.org/proceedings/papers/v22/busoniu12/busoniu12.pdf
sections: 
author:
- given: Lucian
  family: Busoniu
- given: Remi
  family: Munos
date: '2012-03-21 00:03:02'
publisher: PMLR
---
